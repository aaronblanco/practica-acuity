{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teodosiodg2002/practica-acuity/blob/main/PracticaAcuity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d0d0b7",
      "metadata": {
        "id": "83d0d0b7"
      },
      "source": [
        "# Pr√°ctica Acuity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "599e475c",
      "metadata": {
        "id": "599e475c"
      },
      "source": [
        "# Celda 1: Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "e5ae4bdf",
      "metadata": {
        "id": "e5ae4bdf",
        "outputId": "4f9506c0-bf35-4e44-f8ef-390c7fc0d46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Modelado\n",
        "from wurlitzer import sys_pipes\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Configuraci√≥n para mostrar todas las columnas\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9edb7d",
      "metadata": {
        "id": "5e9edb7d"
      },
      "source": [
        "# Celda 2: Carga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "b138ae3d",
      "metadata": {
        "id": "b138ae3d",
        "outputId": "210e8475-e240-475b-e777-51b6bd0d069f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv cargado\n",
            "datos de test cargado\n",
            "datos de entrenamiento cargado\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# CARGA DE DATASETS\n",
        "# ==========================================\n",
        "# Definimos la ruta donde se encuentran los archivos\n",
        "from pathlib import Path\n",
        "DATA_DIR = Path('/content/drive/MyDrive/datos/')\n",
        "\n",
        "# Cargamos cada dataset\n",
        "try:\n",
        "    df_submission = pd.read_csv(DATA_DIR / 'submission.csv')\n",
        "    print(\"submission.csv cargado\")\n",
        "\n",
        "    df_test_data = pd.read_csv(DATA_DIR / 'test_kaggle.csv')\n",
        "    print(\"datos de test cargado\")\n",
        "\n",
        "    df_train_data = pd.read_csv(DATA_DIR / 'train_kaggle.csv')\n",
        "    print(\"datos de entrenamiento cargado\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Archivo no encontrado - {e}\")\n",
        "    print(\"Verifica que todos los archivos est√©n en la carpeta correcta.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado al cargar archivos: {e}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcdb8da",
      "metadata": {
        "id": "bfcdb8da"
      },
      "source": [
        "# CELDA 3: Limpieza Inicial y Selecci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos datos (asumiendo que df_train_data ya est√° cargado como en tu Celda 2)\n",
        "df_clean = df_train_data.dropna(subset=['acuity']).copy()\n",
        "y = df_clean['acuity']\n",
        "\n",
        "# A. Limpieza espec√≠fica de la columna 'pain' (Convertir a num√©rico)\n",
        "def clean_pain(val):\n",
        "    try:\n",
        "        # Intentar convertir a float directo\n",
        "        return float(val)\n",
        "    except:\n",
        "        # Manejar textos como 'unable', 'refused' -> Asignamos NaN para imputar luego\n",
        "        return np.nan\n",
        "\n",
        "df_clean['pain_num'] = df_clean['pain'].apply(clean_pain)\n",
        "\n",
        "# B. Rellenar nulos en texto para evitar errores\n",
        "df_clean['chiefcomplaint'] = df_clean['chiefcomplaint'].fillna('')\n",
        "\n",
        "# C. Definir columnas\n",
        "# NOTA: Ahora incluimos 'chiefcomplaint' y usamos 'pain_num'\n",
        "cols_to_drop = [\n",
        "    'stay_id', 'subject_id', 'hadm_id', 'subject_id_triage',\n",
        "    'intime', 'outtime', 'acuity', 'pain' # Eliminamos pain original, usamos pain_num\n",
        "]\n",
        "\n",
        "X = df_clean.drop(columns=cols_to_drop, errors='ignore')"
      ],
      "metadata": {
        "id": "ZPlpQppuvamY"
      },
      "id": "ZPlpQppuvamY",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELDA 4: Split & Imputaci√≥n (Ingenier√≠a de Datos)\n"
      ],
      "metadata": {
        "id": "NTp8uFQ8vwrE"
      },
      "id": "NTp8uFQ8vwrE"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Definici√≥n de grupos de columnas\n",
        "numeric_features = ['temperature', 'o2sat', 'resprate', 'dbp', 'sbp', 'heartrate', 'pain_num']\n",
        "text_features = 'chiefcomplaint'\n",
        "# Identificar categ√≥ricas restantes autom√°ticamente (excluyendo texto y num√©ricas)\n",
        "categorical_features = [col for col in X.columns if col not in numeric_features and col != text_features]\n",
        "\n",
        "print(f\"Num√©ricas: {len(numeric_features)} | Categ√≥ricas: {len(categorical_features)} | Texto: 1\")\n",
        "\n",
        "# --- PIPELINES DE TRANSFORMACI√ìN ---\n",
        "\n",
        "# 1. Num√©rico: Imputaci√≥n Mediana + Escalado (Ayuda a algunos modelos)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# 2. Categ√≥rico: Imputaci√≥n Constante + OneHot\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# 3. Texto: TF-IDF (Transforma texto en matriz num√©rica de importancia)\n",
        "# max_features=100: Toma las 100 palabras m√°s importantes (dolor, pecho, fiebre, etc.)\n",
        "text_transformer = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "\n",
        "# --- UNIFICACI√ìN (COLUMN TRANSFORMER) ---\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('txt', text_transformer, text_features)\n",
        "    ],\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Aplicamos la transformaci√≥n\n",
        "# Esto genera arrays de numpy, ideal para el modelo\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "# Convertimos a DataFrame solo para visualizaci√≥n/debug (opcional, consume memoria)\n",
        "feature_names = (numeric_features +\n",
        "                 list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out()) +\n",
        "                 list(preprocessor.named_transformers_['txt'].get_feature_names_out()))\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
        "\n",
        "print(f\"Dimensiones finales de X_train: {X_train_df.shape}\")\n",
        "# Ahora tienes ~150 columnas (vitals + onehot + 100 palabras clave)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPazOEJQvvlY",
        "outputId": "e56cf04f-2b70-4e00-d9d5-589844c540aa"
      },
      "id": "OPazOEJQvvlY",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num√©ricas: 7 | Categ√≥ricas: 7 | Texto: 1\n",
            "Dimensiones finales de X_train: (167240, 383)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELDA 5: Modelo Base (Random Forest)\n"
      ],
      "metadata": {
        "id": "BWjsZj0Owa0c"
      },
      "id": "BWjsZj0Owa0c"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inicializar el modelo\n",
        "# n_estimators=100: Crea 100 √°rboles de decisi√≥n\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "# class_weight='balanced': Ayuda al modelo a prestar atenci√≥n a las clases minoritarias\n",
        "\n",
        "# 2. Entrenar (Fit)\n",
        "print(\"Entrenando modelo... (puede tardar unos segundos)\")\n",
        "rf_model.fit(X_train_processed, y_train)\n",
        "\n",
        "# 3. Predecir en Validaci√≥n\n",
        "y_pred = rf_model.predict(X_val_processed)\n",
        "\n",
        "# 4. Evaluaci√≥n de Resultados\n",
        "acc = accuracy_score(y_val, y_pred)\n",
        "print(f\"\\n‚úÖ Accuracy del modelo: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nüìä Reporte de Clasificaci√≥n por Clase:\")\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "# (Opcional) Matriz de Confusi√≥n simple\n",
        "print(\"\\nMatriz de Confusi√≥n (Filas=Realidad, Col=Predicci√≥n):\")\n",
        "print(confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6i3HBBIwZdj",
        "outputId": "c78df9a5-69cf-4dc8-9ae0-b60ec3707853"
      },
      "id": "r6i3HBBIwZdj",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrenando modelo... (puede tardar unos segundos)\n",
            "\n",
            "‚úÖ Accuracy del modelo: 0.6762\n",
            "\n",
            "üìä Reporte de Clasificaci√≥n por Clase:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.78      0.53      0.63      2402\n",
            "         2.0       0.66      0.56      0.60     13941\n",
            "         3.0       0.68      0.84      0.75     22507\n",
            "         4.0       0.58      0.09      0.15      2850\n",
            "         5.0       1.00      0.01      0.02       110\n",
            "\n",
            "    accuracy                           0.68     41810\n",
            "   macro avg       0.74      0.41      0.43     41810\n",
            "weighted avg       0.67      0.68      0.65     41810\n",
            "\n",
            "\n",
            "Matriz de Confusi√≥n (Filas=Realidad, Col=Predicci√≥n):\n",
            "[[ 1274   719   409     0     0]\n",
            " [  325  7798  5812     6     0]\n",
            " [   33  3361 18955   158     0]\n",
            " [    0    26  2581   243     0]\n",
            " [    0     1    93    15     1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELDA 6: PROCESAMIENTO TEST Y SUBMISSION"
      ],
      "metadata": {
        "id": "ehmD3eBEx5mv"
      },
      "id": "ehmD3eBEx5mv"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Procesando Test Set con Pipeline Mejorado ---\")\n",
        "\n",
        "# 1. Crear una copia para trabajar\n",
        "df_test_clean = df_test_data.copy()\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# A. APLICAR LA MISMA LIMPIEZA MANUAL (Feature Engineering)\n",
        "# ---------------------------------------------------------\n",
        "# Es vital aplicar las mismas funciones 'custom' que definimos antes\n",
        "\n",
        "# Limpieza de 'pain' (usando la funci√≥n clean_pain definida en celdas anteriores)\n",
        "df_test_clean['pain_num'] = df_test_clean['pain'].apply(clean_pain)\n",
        "\n",
        "# Limpieza de 'chiefcomplaint' (rellenar nulos)\n",
        "df_test_clean['chiefcomplaint'] = df_test_clean['chiefcomplaint'].fillna('')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# B. PREPARAR X_test\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Guardamos los IDs para el archivo final\n",
        "submission_ids = df_test_clean['stay_id']\n",
        "\n",
        "# Eliminamos las columnas que no usa el modelo (misma lista cols_to_drop)\n",
        "# Nota: 'acuity' no existe en test, y 'pain' lo borramos porque ya creamos 'pain_num'\n",
        "X_test = df_test_clean.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# C. TRANSFORMACI√ìN (PIPELINE)\n",
        "# ---------------------------------------------------------\n",
        "print(\"Aplicando transformaciones (Pipeline)...\")\n",
        "\n",
        "# CR√çTICO: Usamos .transform(), NUNCA .fit() en datos de test.\n",
        "# Esto asegura que el modelo reciba las columnas en el orden exacto que espera\n",
        "# y use el mismo vocabulario para el texto.\n",
        "try:\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    print(f\"Dimensiones de X_test procesado: {X_test_processed.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error en la transformaci√≥n: {e}\")\n",
        "    print(\"Aseg√∫rate de haber ejecutado la celda de entrenamiento (Fit) primero.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# D. PREDICCI√ìN Y EXPORTACI√ìN\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "if 'rf_model' in locals():\n",
        "    print(\"Generando predicciones...\")\n",
        "\n",
        "    # Predecir usando la matriz procesada\n",
        "    predictions = rf_model.predict(X_test_processed)\n",
        "\n",
        "    # Crear DataFrame final\n",
        "    df_final = pd.DataFrame({\n",
        "        'stay_id': submission_ids,\n",
        "        'acuity': predictions\n",
        "    })\n",
        "\n",
        "    # Verificaci√≥n r√°pida\n",
        "    print(\"\\nVista previa de predicciones:\")\n",
        "    print(df_final['acuity'].value_counts().sort_index())\n",
        "\n",
        "    # Guardar a CSV\n",
        "    output_filename = 'submission_mejorado.csv'\n",
        "    df_final.to_csv(output_filename, index=False)\n",
        "    print(f\"\\n‚úÖ Archivo '{output_filename}' generado exitosamente.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è El modelo 'rf_model' no est√° definido. Ejecuta la celda de entrenamiento primero.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03ojB0270har",
        "outputId": "1b4ac7e3-32c2-45d5-aced-74d56b08dc05"
      },
      "id": "03ojB0270har",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Procesando Test Set con Pipeline Mejorado ---\n",
            "Aplicando transformaciones (Pipeline)...\n",
            "Dimensiones de X_test procesado: (209050, 383)\n",
            "Generando predicciones...\n",
            "\n",
            "Vista previa de predicciones:\n",
            "acuity\n",
            "1.0      8009\n",
            "2.0     59460\n",
            "3.0    139406\n",
            "4.0      2172\n",
            "5.0         3\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úÖ Archivo 'submission_mejorado.csv' generado exitosamente.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}